                                                             An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale


We were first introduced to the concept of transformers in the field of natural language processing through Vaswani's paper [1]. During the discussion of the CLIP paper by Alec Radford and his team [2], we gained a deeper understanding of visual transformers used for image recognition. Our curiosity about the transition from NLP-based transformers to visual transformers motivated us to explore related research papers, ultimately leading us to choose this paper as the focus of our project. This aligns directly with our research interests and goals because it empowers us to explore the convergence of different data types and the development of multi-modal models, which are central to our current initiatives.

Significance: The paper "An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" [3] is significant for revolutionizing computer vision by applying the transformer architecture, originally designed for natural language processing, to image recognition tasks. This approach, known as Vision Transformer (ViT), achieves competitive or superior performance on various image classification tasks, showcasing the versatility and scalability of transformers in different domains beyond text processing. The success of ViT model in computer vision also led to the broader exploration for integrating different modalities into a single unified model called Multimodal Model.


[1] Vaswani (2017), Attention Is All You Need
[2] Radford (2021), Learning Transferable Visual Models From Natural Language Supervision
[3] Dosovitskiy (2021), An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale
