<!doctype html>
<html lang="en">
<head>
<title>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
<meta property="og:title" content="An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" />
<meta name="twitter:title" content="An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" />
<meta name="description" content="Exploring the transformative approach to image recognition using Transformers." />
<meta property="og:description" content="Exploring the transformative approach to image recognition using Transformers." />
<meta name="twitter:description" content="Exploring the transformative approach to image recognition using Transformers." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" />
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">

</head>
<body class="nd-docs">
  <nav class="navbar navbar-expand-lg navbar-light bg-light">
    <a class="navbar-brand" href="#">CS 7150 Project</a>
    <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="navbar-toggler-icon"></span>
    </button>
    <div class="collapse navbar-collapse" id="navbarNav">
        <ul class="navbar-nav">
            <li class="nav-item active">
                <a class="nav-link" href="#literature-review">Literature Review <span class="sr-only">(current)</span></a>
            </li>
            <li class="nav-item">
                <a class="nav-link" href="#author-biographies">Authors' Backgrounds</a>
            </li>
            <li class="nav-item">
                <a class="nav-link" href="#diagram-section">Diagrams</a>
            </li>
            <li class="nav-item">
                <a class="nav-link" href="#social-impact">Societal Impact</a>
            </li>
            <li class="nav-item">
              <a class="nav-link" href="#industry-app">Industry Appliction</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#followup">Followup-Research</a>
          </li>
          <li class="nav-item">
            <a class="nav-link" href="#references">References</a>
        </li>
        </ul>
    </div>
</nav>
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">An Image is Worth 16x16 Words</nobr>
 <nobr class="widenobr">Transformers for Image Recognition at Scale</nobr>
 <nobr class="widenobr">For CS 7150</nobr>
 </h1>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="row">
<div class="col justify-content-center text-center">
<h2>An Analysis of 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'</h2>
<p>Exploring how Transformers revolutionize image recognition, offering a novel perspective in computer vision.</p>
</div>
</div>
<div class="row">
<div class="col">

<div id="literature-review" class="content-section">
    <h3>Literature Review</h3>
    <p><strong>Preceding Works:</strong></p>
    <ul>
        <li><strong>Transformers for NLP by Vaswani et al. (2017):</strong> Introduced the concept of Transformers, revolutionizing NLP with a novel attention mechanism.</li>
        <li><strong>BERT (Devlin et al., 2019):</strong> Advanced the field of NLP by introducing a new method for pre-training language representations, inspiring the use of Transformers in other domains.</li>
    </ul>

    <p><strong>Subsequent Influences:</strong></p>
    <ul>
        <li><strong>"Not All Images are Worth 16x16 Words: Dynamic Transformers for Efficient Image Recognition" by Yulin Wang et al.:</strong> Proposed Dynamic Transformers to improve computational efficiency in image recognition by adaptively configuring token numbers for each image.</li>
    </ul>

    <p><strong>Historical Significance:</strong></p>
    <p>This paper is significant for its innovative approach of applying Transformer models, traditionally used in NLP, to image recognition, indicating a potential paradigm shift in computer vision methodologies.</p>
    
</div>



<div id="author-biographies" class="content-section">
  <h3>Authors' Backgrounds and Motivations</h3>
  <p>The common thing aiming all the authors was that they were part of the Google research, Brain Team. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, and Neil Houlsby equally contributed in advising for the paper. Alexey Dosovitskiy and Neil Houlsby's equal contribution in technical aspect.</p>

  <div class="author-container">
      <div class="author">
          <img src="alexey_photo.jpg" alt="Alexey Dosovitskiy" class="author-image">
          <h4>Alexey Dosovitskiy</h4>
          <p>Born and educated in Moscow, Russia. Focused on neural networks, and unsupervised feature learning. Works at Intel Visual Computing Lab, Munich</p>
      </div>

      <div class="author">
        <img src="lucas_beyer.jpeg" alt="Lucas Beyer" class="author-image">
        <h4>Lucas Beyer</h4>
        <p>Pursued his PhD in Computer Science from RWTH Aachen University. Research interests include Representation Learning, Computer Vision, Robotics.Staff Research Engineer at Google.</p>
      </div>

      <div class="author">
        <img src="alexader.jfif" alt="Alexander Kolesnikov" class="author-image">
        <h4>Alexander Kolesnikov</h4>
        <p>Pursued his PhD in Machine Learning and Computer Vision from Institute of Science and Technology, Austria. Research interests include  AI, Machine learning, Deep learning, Computer vision. Staff Research Engineer at Google DeepMind.</p>
      </div>

      <div class="author">
        <img src="Dirk.jfif" alt="Dirk Weissenborn" class="author-image">
        <h4>Dirk Weissenborn</h4>
        <p>Pursued his PhD in German Research Center for AI. Research interests include Deep Learning, Representation Learning, NLP, Information Extraction. Former Research Scientist at Google. Currently working as a Technical Staff at Inceptive.</p>
      </div>

      <div class="author">
          <img src="zhai.jpg" alt="Xiaohua Zhai" class="author-image">
          <h4>Xiaohua Zhai</h4>
          <p>Senior staff researcher at Google Deepmind, Switzerland. Received Ph.D. degree in Computer Science from Peking University. Working on deep learning and computer vision.​</p>
          </ul>
      </div>

      <div class="author">
        <img src="Georg-Heigold.jpg" alt="Georg Heigold" class="author-image">
        <h4>Georg Heigold</h4>
        <p>Diploma degree in physics from ETH Zurich. Former Software Engineer, now a Research Scientist at Google. Interests include automatic speech recognition and discriminative training.</p>
      </div>

      <div class="author">
        <img src="jakob_1.png" alt="Jakob Uszkoreit" class="author-image">
        <h4>Jakob Uszkoreit</h4>
        <p>Studied in Berlin, joined Google in 2008, and later co-founded Inceptive in 2021, focusing on biological software for new medicines and biotechnologies.</p>
      </div>

      <div class="author">
        <img src="neil.jpg" alt="Neil Houlsby" class="author-image">
        <h4>Neil Houlsby</h4>
        <p>Senior Research Scientist at Google Brain, Zürich. Works on Machine Learning, transfer learning, representation learning, AutoML, computer vision, and NLP. PhD from the Cambridge CBL Laboratory.</p>

    </div>
</div>


<div id="diagram-section" class="content-section">
  <h3>Diagrams</h3>
  <div class="diagram">
      <img src="vit_architecture.jpg" alt="Diagram 1" class="diagram-image">
      <p class="diagram-description">Description of Diagram 1: The model, shown here, simplifies handling 2D images for the Transformer, originally designed for 1D data. Images are split into fixed-size patches, linearly embed each of them, add position embeddings and feed the resulting sequence of vectors to a standard Transformer. A special token, similar to BERT's [class] token, is added to this sequence, as an extra learnable 'classification token' to the sequence. The model uses learnable position embeddings to maintain spatial information. The architecture includes layers of self-attention and MLP blocks, with specific norms and residual connections for effective learning. Unlike CNNs, the Vision Transformer has less image-specific structure, learning spatial relationships from scratch, with only minimal use of the two-dimensional structure. </p>
  </div>

  <div class="diagram">
      <img src="vitperformance.png" alt="Diagram 2" class="diagram-image">
      <p class="diagram-description">Description of Diagram 2: The Vision Transformer (ViT) shows improved performance with larger datasets. While larger ViT models underperform on smaller datasets like ImageNet, they excel with larger datasets like JFT-300M. In contrast, traditional convolutional models like ResNets perform better on smaller datasets. This suggests that ViT models are more suitable for scenarios with access to large amounts of data, while conventional models may be preferable for smaller datasets.</p>
  </div>
  
  <div class="diagram">
    <img src="process.png" alt="Diagram 2" class="diagram-image">
    <p class="diagram-description">Description of Diagram 3: The left figure illustrates the linear embeddings of RGB values, resembling the kernels learned by CNNs. In the center, the figure displays the appearance of 1D positional embeddings post the model's pre-training. This depiction showcases the model's ability to encode distance within the image, as closer patches exhibit simialr position embeddings. As for the figure on the right, it represents the analysis of the large variant model with different attention heads - specifically, a model consisting of 24 layers and 16 attention heads. Generally, an increase in attention heads leads to greater network depth. The graph demonstrates that, in transformers, even in the shallower layers, attention spans globally, contrary to CNNs where attention transitions from local to global as depth increases.</p>
  </div>

</div>


<div id="social-impact" class="content-section">
  <h3>Societal Impact of Transformers in Image Recognition</h3>

  <h4>Positive Impacts:</h4>
  <ul>
      <li>Innovation in Computer Vision: Introduction of Vision Transformers (ViT) shifts image recognition technology.</li>
      <li>Efficiency in Data Processing: ViT demonstrates improved performance in image recognition with less computational demand.</li>
  </ul>

  <h4>Negative Impacts:</h4>
  <ul>
      <li>Potential for Misuse: Risk of misuse in surveillance and privacy violations.</li>
      <li>Dependence on Large Datasets: Reliance on large datasets for training could perpetuate existing biases in AI.</li>
  </ul>

  <p><strong>Recommendation for Policymakers:</strong> Create regulations for ethical use of image recognition technologies, addressing privacy and bias issues, and encouraging transparency and unbiased data collection.</p>
</div>

<div id ="industry-app" class="content-section">
  <h3>Industry Applications of Vision Transformers</h3>

  <p><strong>Real-World Problem/Scenario:</strong></p>
  <ul>
      <li>Healthcare Diagnostics: Enhancing medical imaging analysis for faster and more accurate disease diagnosis.</li>
  </ul>

  <p><strong>Potential Benefits:</strong></p>
  <ul>
      <li>Efficiency in Manufacturing: Automated quality control and defect detection.</li>
      <li>Retail and E-commerce: Improved product recommendation systems and visual search.</li>
  </ul>

  <p><strong>Challenges and Considerations:</strong></p>
  <ul>
      <li>Data Privacy: Protecting sensitive information, especially in healthcare applications.</li>
      <li>Computational Resources: Requirement for significant computational power for training and deployment.</li>
      <li>Bias and Fairness: Ensuring unbiased outcomes by addressing potential biases in training data.</li>
  </ul>
</div>

<div id ="followup" class="content-section">
  <h3>Proposed Follow-Up Research</h3>
      <p>We can research on efficiency of transformers by incorporating changes to the attention mechanisms, normalization layers, and positional encodings. Also, further research can be done on improving the efficiency by reducing the computational cost of self-supervised learning for ViTs.</p>
</div>

<div id='review' class="content-section">
  
    <h3>Reviews</h3>
     <p><strong>Debankita Basu</strong></p>
    
    <p><strong>Summary:</strong></p>
    <p>The paper introduces Vision Transformer (ViT), an innovative application of Transformer architecture in image recognition. It processes images by dividing them into 16x16 patches, treating each as a 'word'. This method demonstrates the potential of Transformers in computer vision, challenging the dominance of CNNs in this field.</p>
  
    <p><strong>Strengths and Weaknesses:</strong></p>
    <p><strong>Originality:</strong></p>
    <ul>
      <li><strong>Strength:</strong> Innovative application of Transformer architecture to image recognition.</li>
      <li><strong>Weakness:</strong> Core technology not novel, adaptation of existing methods.</li>
    </ul>
    
    <p><strong>Quality:</strong></p>
    <ul>
      <li><strong>Strength:</strong> Technically robust, well-supported by experiments.</li>
      <li><strong>Weakness:</strong> Requires exploration in resource-constrained environments.</li>
    </ul>
  
    <p><strong>Clarity:</strong></p>
    <ul>
      <li><strong>Strength:</strong> Articulate and well-organized, making complex concepts accessible.</li>
      <li><strong>Weakness:</strong> Some sections may require additional clarity.</li>
    </ul>
  
    <p><strong>Significance:</strong></p>
    <ul>
      <li><strong>Strength:</strong> Offers a new perspective in image recognition, influencing future research.</li>
      <li><strong>Weakness:</strong> Practical application may be limited due to dataset and computational requirements.</li>
    </ul>
  
    <p><strong>Questions and Suggestions:</strong></p>
    <ul>
      <li>How does ViT perform across different image datasets, especially smaller ones?</li>
      <li>Comments on the computational efficiency of ViT compared to CNNs?</li>
      <li>What improvements or adaptations could enhance ViT's applicability?</li>
    </ul>
  
    <p><strong>Limitations and Societal Impact:</strong></p>
    <p>Discussion on limitations, particularly dataset size and computational demands, is needed. Consideration of societal implications, such as biases in image recognition systems, is crucial.</p>
  
    <p><strong>Ethical Concerns:</strong></p>
    <p>No direct ethical concerns identified, but an ethics review on potential biases and misuse in surveillance or data privacy would be appropriate.</p>
  
    <p><strong>Ratings:</strong></p>
    <ul>
      <li><strong>Soundness:</strong> 4 - Well-supported technical claims.</li>
      <li><strong>Presentation:</strong> 3 - Clear, with minor improvements needed.</li>
      <li><strong>Contribution:</strong> 3 - Significant and original contribution to the field.</li>
      <li><strong>Overall Score:</strong> 7 - Technically solid, high-impact paper with good evaluation.</li>
      <li><strong>Confidence:</strong> 4 - Confident in assessment, with some limitations in understanding.</li>
    </ul>

    <p> <strong>Sravani Namburu</strong></p>
    <p><strong>Summary:</strong></p>
    <p>The paper introduces Vision Transformer (ViT), similar to transformer for Natural language processing tasks. It processes the images by diving the image into patches, similar to tokens in NLP tasks.</p>
  
    <p><strong>Strengths and Weaknesses:</strong></p>
    <p><strong>Originality:</strong></p>
    <ul>
      <li><strong>Strength:</strong> The strength lies in the idea of applying the Transformer architecture, originally prominent in NLP tasks, to the domain of image recognition.</li>
      <li><strong>Weakness:</strong> No in-depth analysis on Inductive Bias and how the model generalizes to various types of images, different datasets.</li>
    </ul>
    
    <p><strong>Quality:</strong></p>
    <ul>
      <li><strong>Strength:</strong> Technically strong and promising performance results.</li>
      <li><strong>Weakness:</strong> Lack of in-depth examination of learned representations.</li>
    </ul>
  
    <p><strong>Clarity:</strong></p>
    <ul>
      <li><strong>Strength:</strong> Evaluation is very-well presented.</li>
      <li><strong>Weakness:</strong> More work on self-supervised learning would have been appreciated.</li>
    </ul>
  
    <p><strong>Significance:</strong></p>
    <ul>
      <li><strong>Strength:</strong> The significant exploration of transformers in image recognition tasks, led to many further research opportunities, emphasizing its substantial strength in advancing the field.</li>
      <li><strong>Weakness:</strong> The study clearly indicates that the performance of transformer increases with the dataset size. But, there might be some situations where data required could be very sparse. </li>
    </ul>
  
    <p><strong>Questions and Suggestions:</strong></p>
    <ul>
      <li>Will there be any saturating point for scaling the transformers?</li>
      <li>Instead of treating image like a sentence, by dividing the image into patches, can we implement transformer based models on a whole image?</li>
    </ul>
  
    <p><strong>Limitations and Societal Impact:</strong></p>
    <p>Size of the dataset required could be one of the limitations, to improve the performance of the transformer. Apart from limitations, this paper led to have positive societal imapct, leading to further improvements in image recognition like, facial recognition and medical image analysis.</p>

    <p><strong>Ethical Concerns:</strong></p>
    <p>Privacy violations, and potential misuse of this technology like DeepFake or manipulating information.</p>
  
    <p><strong>Ratings:</strong></p>
    <ul>
      <li><strong>Soundness:</strong> 4 - Well-supported claims, well-designed methodology, experiments and robust results.</li>
      <li><strong>Presentation:</strong> 3 - Clear and mostly well-organized, with minor issues with clarity.</li>
      <li><strong>Contribution:</strong> 4 - Significant and impactful contribution to the field, presenting new perspectives.</li>
      <li><strong>Confidence:</strong> 3 - Overall confidence is reasonable, with some uncertainities about certain aspects of the paper.</li>
      <li><strong>Overall Score:</strong> 8 - Technically solid, high-impact paper with good evaluation.</li>
    </ul>

  </div>
  
  


<div id ="references" class="content-section">
  <h3>References</h3>

  <p><a name="vaswani-2017">[1]</a> <a href="https://arxiv.org/pdf/1706.03762.pdf"
    >Transformers for NLP by Vaswani et al.
    </a>
    Attention Is All You Need (2017).
  </p>

  <p><a name="devlin-2019">[2]</a> <a href="https://aclanthology.org/N19-1423.pdf"
    >BERT: 
    </a>
    Pre-training of Deep Bidirectional Transformers for
    Language Understanding (2017).
  </p>
</div>


<h2>Team Members</h2>
                                                  
<p>Debankita Basu</p>
<p>Sravani Namburu</p>

  
</div><!--col-->
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-12 text-center">
      <a href="https://cs7150.baulab.info/">About CS 7150</a>
    </div>
  </div>
</footer>

</body>
<style>
  html {
      scroll-behavior: smooth;
  }
</style>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>
