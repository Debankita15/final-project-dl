<!doctype html>
<html lang="en">
<head>
<title>An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale</title>
<meta property="og:title" content="An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" />
<meta name="twitter:title" content="An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale" />
<meta name="description" content="Exploring the transformative approach to image recognition using Transformers." />
<meta property="og:description" content="Exploring the transformative approach to image recognition using Transformers." />
<meta name="twitter:description" content="Exploring the transformative approach to image recognition using Transformers." />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" /> 
<meta name="viewport" content="width=device-width,initial-scale=1" />
<!-- bootstrap for mobile-friendly layout -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/css/bootstrap.min.css" integrity="sha384-xOolHFLEh07PJGoPkLv1IbcEPTNtaed2xpHsD9ESMhqIYd0nLMwNLD69Npy4HI+N" crossorigin="anonymous">
<script src="https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.slim.min.js" integrity="sha384-DfXdz2htPH0lsSSs5nCTpuj/zy4C+OGpamoFVy38MVBnE+IbbVYUew+OrCXaRkfj" crossorigin="anonymous"></script>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.2/dist/js/bootstrap.bundle.min.js" integrity="sha384-Fy6S3B9q64WdZWQUiU+q4/2Lc9npb8tCaSX9FK7E8HnRr0Jz8D6OP9dO5Vg3Q9ct" crossorigin="anonymous"></script>
<link href="https://fonts.googleapis.com/css?family=Open+Sans:300,400,700" rel="stylesheet">
<link href="style.css" rel="stylesheet">

</head>
<body class="nd-docs">
<div class="nd-pageheader">
 <div class="container">
 <h1 class="lead">
 <nobr class="widenobr">An Image is Worth 16x16 Words</nobr>
 <nobr class="widenobr">Transformers for Image Recognition at Scale</nobr>
 <nobr class="widenobr">For CS 7150</nobr>
 </h1>
 </div>
</div><!-- end nd-pageheader -->

<div class="container">
<div class="row">
<div class="col justify-content-center text-center">
<h2>An Analysis of 'An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale'</h2>
<p>Exploring how Transformers revolutionize image recognition, offering a novel perspective in computer vision.</p>
</div>
</div>
<div class="row">
<div class="col">

<div class="literature-review">
    <h3>Literature Review</h3>
    <p><strong>Preceding Works:</strong></p>
    <ul>
        <li><strong>Transformers for NLP by Vaswani et al. (2017):</strong> Introduced the concept of Transformers, revolutionizing NLP with a novel attention mechanism.</li>
        <li><strong>BERT (Devlin et al., 2019):</strong> Advanced the field of NLP by introducing a new method for pre-training language representations, inspiring the use of Transformers in other domains.</li>
    </ul>

    <p><strong>Subsequent Influences:</strong></p>
    <ul>
        <li><strong>"Not All Images are Worth 16x16 Words: Dynamic Transformers for Efficient Image Recognition" by Yulin Wang et al.:</strong> Proposed Dynamic Transformers to improve computational efficiency in image recognition by adaptively configuring token numbers for each image.</li>
    </ul>

    <p><strong>Historical Significance:</strong></p>
    <p>This paper is significant for its innovative approach of applying Transformer models, traditionally used in NLP, to image recognition, indicating a potential paradigm shift in computer vision methodologies.</p>
    
</div>



<div class="author-biographies">
  <h3>Authors' Backgrounds and Motivations</h3>

  <div class="author-container">
      <div class="author">
          <img src="alexey_photo.jpg" alt="Alexey Dosovitskiy" class="author-image">
          <h4>Alexey Dosovitskiy</h4>
          <p>Born and educated in Moscow, Russia. Focused on neural networks, unsupervised feature learning. Works at Intel Visual Computing Lab, Munich</p>
      </div>

      <div class="author">
          <img src="zhai.jpg" alt="Xiaohua Zhai" class="author-image">
          <h4>Xiaohua Zhai</h4>
          <p>Senior staff researcher at Google Deepmind, Switzerland. Received Ph.D degree in Computer Science from Peking University. Working on deep learning and computer vision.​</p>
          </ul>
      </div>

      <div class="author">
        <img src="Georg-Heigold.jpg" alt="Georg Heigold" class="author-image">
        <h4>Georg Heigold</h4>
        <p>Diploma degree in physics from ETH Zurich. Former Software Engineer, now a Research Scientist at Google. Interests include automatic speech recognition and discriminative training.</p>
      </div>

      <div class="author">
        <img src="jakob_1.png" alt="Jakob Uszkoreit" class="author-image">
        <h4>Jakob Uszkoreit</h4>
        <p>Studied in Berlin, joined Google in 2008, and later co-founded Inceptive in 2021, focusing on biological software for new medicines and biotechnologies.</p>
      </div>

      <div class="author">
        <img src="neil.jpg" alt="Neil Houlsby" class="author-image">
        <h4>Neil Houlsby</h4>
        <p>Senior Research Scientist at Google Brain, Zürich. Works on Machine Learning, transfer learning, representation learning, AutoML, computer vision, and NLP. PhD from the Cambridge CBL Laboratory.</p>

    </div>
</div>


<div class="diagram-section">
  <h3>Diagrams</h3>
  <div class="diagram">
      <img src="vit_architecture.jpg" alt="Diagram 1" class="diagram-image">
      <p class="diagram-description">Description of Diagram 1: The model, shown ihere, simplifies handling 2D images for the Transformer, originally designed for 1D data. Images are split into smaller patches, reshaped into a sequence that the Transformer can process. Each patch is then linearly projected to a specific dimension. A special token, similar to BERT's [class] token, is added to this sequence, and its final state represents the image. The model uses learnable position embeddings to maintain spatial information. The architecture includes layers of self-attention and MLP blocks, with specific norms and connections for effective learning. Unlike CNNs, the Vision Transformer has less image-specific structure, learning spatial relationships from scratch, with only minimal use of the two-dimensional structure. </p>
  </div>

  <div class="diagram">
      <img src="vitperformance.png" alt="Diagram 2" class="diagram-image">
      <p class="diagram-description">Description of Diagram 2: The Vision Transformer (ViT) shows improved performance with larger datasets. While larger ViT models underperform on smaller datasets like ImageNet, they excel with larger datasets like JFT-300M. In contrast, traditional convolutional models like ResNets perform better on smaller datasets. This suggests that ViT models are more suitable for scenarios with access to large amounts of data, while conventional models may be preferable for smaller datasets.</p>
  </div>
  
  <div class="diagram">
    <img src="process.png" alt="Diagram 2" class="diagram-image">
    <p class="diagram-description">Description of Diagram 3: The Vision Transformer begins by transforming image patches into simpler, lower-dimensional representations. It also learns to understand the position of these patches in the image. Patches that are close together tend to have similar representations. The model has the capability to integrate information across the entire image from the beginning, unlike other models that focus on small areas first. Some parts of the model focus on nearby areas, similar to early layers in traditional models, while as it processes deeper, it expands its focus to larger regions of the image.
    </p>
  </div>

</div>


<div class="social-impact">
  <h3>Societal Impact of Transformers in Image Recognition</h3>

  <h4>Positive Impacts:</h4>
  <ul>
      <li>Innovation in Computer Vision: Introduction of Vision Transformers (ViT) shifts image recognition technology.</li>
      <li>Efficiency in Data Processing: ViT demonstrates improved performance in image recognition with less computational demand.</li>
  </ul>

  <h4>Negative Impacts:</h4>
  <ul>
      <li>Potential for Misuse: Risk of misuse in surveillance and privacy violations.</li>
      <li>Dependence on Large Datasets: Reliance on large datasets for training could perpetuate existing biases in AI.</li>
  </ul>

  <p><strong>Recommendation for Policymakers:</strong> Create regulations for ethical use of image recognition technologies, addressing privacy and bias issues, and encouraging transparency and unbiased data collection.</p>
</div>

<div class="industry-application">
  <h3>Industry Applications of Vision Transformers</h3>

  <p><strong>Real-World Problem/Scenario:</strong></p>
  <ul>
      <li>Healthcare Diagnostics: Enhancing medical imaging analysis for faster and more accurate disease diagnosis.</li>
  </ul>

  <p><strong>Potential Benefits:</strong></p>
  <ul>
      <li>Efficiency in Manufacturing: Automated quality control and defect detection.</li>
      <li>Retail and E-commerce: Improved product recommendation systems and visual search.</li>
  </ul>

  <p><strong>Challenges and Considerations:</strong></p>
  <ul>
      <li>Data Privacy: Protecting sensitive information, especially in healthcare applications.</li>
      <li>Computational Resources: Requirement for significant computational power for training and deployment.</li>
      <li>Bias and Fairness: Ensuring unbiased outcomes by addressing potential biases in training data.</li>
  </ul>
</div>

<div class="follow-up-research">
  <h3>Proposed Follow-Up Research</h3>
      <p>We can research on efficiency of transformers by incorporating changes to the attention mechanisms, normalization layers, and positional encodings. Also, further research can be done on improving the efficiency by reducing the computational cost of self-supervised learning for ViTs.</p>
</div>



<h3>References</h3>

<p><a name="vaswani-2017">[1]</a> <a href="https://arxiv.org/pdf/1706.03762.pdf"
  >Transformers for NLP by Vaswani et al.
  </a>
  Attention Is All You Need (2017).
</p>

<p><a name="devlin-2019">[2]</a> <a href="https://aclanthology.org/N19-1423.pdf"
  >BERT: 
  </a>
  Pre-training of Deep Bidirectional Transformers for
  Language Understanding (2017).
</p>

<h2>Team Members</h2>
                                                   
<p>Debankita Basu</p>
<p>Sravani Namburu</p>

  
</div><!--col-->
</div><!--row -->
</div> <!-- container -->

<footer class="nd-pagefooter">
  <div class="row">
    <div class="col-6 col-md text-center">
      <a href="https://cs7150.baulab.info/">About CS 7150</a>
    </div>
  </div>
</footer>

</body>
<script>
$(document).on('click', '.clickselect', function(ev) {
  var range = document.createRange();
  range.selectNodeContents(this);
  var sel = window.getSelection();
  sel.removeAllRanges();
  sel.addRange(range);
});
// Google analytics below.
window.dataLayer = window.dataLayer || [];
</script>
</html>
